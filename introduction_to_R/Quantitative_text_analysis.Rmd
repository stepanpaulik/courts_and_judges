---
title: "Quantitative_text_analysis_1"
author: "Štěpán Paulík"
date: "2023-06-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
# Install packages
# install.packages("tidyverse")
# install.packages("lubridate")
# install.packages("tidytext")
# install.packages("quanteda")
# install.packages("quanteda.textstats")
# install.packages("quanteda.textmodels")
# install.packages("ldatuning")
# install.packages("seededlda")
# install.packages("udpipe")
# install.packages("parallel")



# Load packages
library(parallel)
library(tidyverse)
library(lubridate)
library(tidytext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)
library(ldatuning)
library(seededlda)
library(udpipe)


# Load data dynamically from a github page (should've done this earlier)
load(url("https://github.com/jfjelstul/law-and-courts-workshop/raw/master/data/text_corpus.RData")) %>% as_tibble()
```

We are again working with a RMarkdown file so I hope by now you are familiar with it :) I also figured out a way how to load data directly from Github so there is no more need to hassle with the working directoy. Just run the first chunk and you'll be fine.

Today we will be working with multiple various datasets (in Czech, English and German too). The German data are courtesy to [Sean Fobbe](https://zenodo.org/record/5514083), the English to [Joshua Fjelstul](https://github.com/jfjelstul/law-and-courts-workshop/tree/master) and the Czech obviously to me :). 

## Natural Language Processing / Quantitative Text Analysis
My dataset of Czech apex court decisions has roughly 300 000 decisions including their text. For my research work, it would be impossible to analyse all of them manually. But it's not impossible to use the help of computers. There are countless ways to process text on large scale that also depend on what you want to achieve (data mining, classification...), some are very simple (like regular expressions), other require incredible amount of computing power (large language models like ChatGPT). Quantitative text analysis serves to achieve exactly that purpose.

There are multiple assumptions that need to be met in order for the quantitative approaches to work:

(1) The content of the text (words, sentences) reveals a meaning that we're interested in quantitatively analysing (i.e., it's hard to process something that's not in the text - purpose...)
(2) There must be a way to represent the text in a way that a computer can process it (tokens, lemmas, n-grams, document feature matrix, vectors etc.)
(3) A lot of algorithms are based around relative distribution of words capturing some meaningful variation in something (whether it's proximity in meaning, topics etc.) but not all do or some try to overcome this assumption.

The tasks that you can achieve with quantitative text analysis often comprise of the following purposes:

(1) Frequency analysis (how common is some occurance)
(2) Similarity (are words, paragraphs, documents similar)
(3) Scale (find a, say, left-right scale based on speeches in Parliament)
(4) Classification (inference about belonging of an element to certain class - classify a structure of a judicial decision)
(5) Generation (just see the incredible advances in generative AI... hard to imagine use for research though)

In the following sections, we will delve into:

(1) Regular Expressions
(2) Topic Modelling with LDA
(3) Scaling with Wordfish
(4) ... and most interestingly to the supervised classification algorithms (SVM, random forests and xgboosted trees)

## Regular expressions
We will now start with the simplest form of processing text, regular expressions ("RegEx"). RegEx allow you to search for patterns in text that you know are there. In more specific terms, we have a RegEx pattern to search for in any given string (or multiple strings). We can then match (*str_detect()*, which returns TRUE/FALSE if it finds the pattern in the string), remove (*str_remove()*), or extract (*str_extract()*) the patterns we search.

The advantage of RegEx is that it is computationally very cheap (doesn't demand a lot of computational power/time) and simple to use. The disadvantage is that it requires high level of regularity of the information you are looking for.

For the starts, you can match the pattern literally.
``` {r regex_simple} 
string = "courts and judges"
string %>%
  str_extract(pattern = "courts")

string %>%
  str_detect(pattern = "duck")
```

However, that's most of the time not very useful. Imagine you want to match a bunch of names. We know names consist of two words, both of which start with a upper-case letter. RegEx allows you to generalize such demands.

[A-Z] stand for upper-case letters, [a-z] for lowerę-case letters, [0-9] stands for numbers (digit), /s for space etc. For the complete list consult the assigned chapter.

``` {r regex_name}
# Run the first
wiki = data.frame(wiki_text = c("Lee Epstein is a political scientist", "Arnold Schwarzenegger is an american actor"))

# And then run this and try to understand what just happened
wiki = string %>%
  mutate(
    name = wiki_text %>%
      str_extract(pattern = "[A-Z][a-z]+\\s[A-Z][a-z]+")
  )
```

I made use of the so called quantifiers with the *+* symbol. They allow you to tell the RegEx how many times will a certain preceding expression repeat. 

* means zero or more
+ means one or more
? means zero or one
{n,m} can be used as a range between n and m occurrences.

The quantifiers allow us to deal with more irregularities. Imagine the following text, where the last name is not in the same for as the first two. How do we deal with that? 

*Exercise 1*
Replace the *#* with the quantifiers that will catch all the names (a hint is that each of the three quantifiers is used exactly once).
``` {r regex_name_exercise, eval = FALSE}
# Run the first
wiki = data.frame(wiki_text = c("Lee Epstein is a political scientist", "Arnold Schwarzenegger is an american actor", "D. Trump is a former US president"))

# And then run this and try to understand what just happened
wiki = wiki %>%
  mutate(
    name = wiki_text %>%
      str_extract(pattern = "[A-Z]\\.#[a-z]#\\s[A-Z][a-z]#")
  )
```
There is so much you can do with regex that we can't cover here. But as always programming is best taught via problem solving problems you yourself are interested in overcoming rather than lecture style teaching.

**Exercise 2**
Take the following chunk of text and try to extract the name of the judge rapporteur (in fact the author of the decision). Read all three excerpts carefully, try to see which pattern you can use to "anchor" the name of the judge rapporteur extraction and try to translate it into a regex pattern.

``` {r exercise_2}
# This is the data you will be working with
judge_rapporteur = text_corpus %>%
  select(ecli, text) %>%
  filter(str_detect(text, "^composed of")) %>%
  as_tibble() %>%
  slice_tail(n = 3)

# Your code goes here
```

**Exercise 3**
Look into the NSS_lawyers data.frame. There you will find one column with the name of the lawyer representing a party before the Czech Supreme Administrative Court, including their title, and information whether the case was granted or not. As a first step, try to split the first column into one that contains only the name of the lawyer and one that contains their title and then transform the title into simple binary 1/0 if the titles contain PhD or don't. Lastly, build a simple binomial logit model that ascertains the influence of the PhD on your lawyers' winning chances. If you are feeling 

``` {r exercise_3}
NSS_df = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/NSS_df.rds"))

# Check the ability distribution
NSS_df %>% 
  ggplot() +
  geom_density(aes(x = ability, group = phd, color = phd))

# The model
logit = glm(outcome ~ phd, data = NSS_df, family = "binomial")

plot(logit)
```

### Text normalization / UDPipe
Processing large amount of text contains multiple domain specific "knowledges": you have to know the substance of the text, computer science as well as some basic understanding of the language and its linguistics. The linguistic rules of languages poses countless problem.

To process any large amount of text, you need to be able to represent the text in a computer readable way. The basic question is what will be represented. In almost any case, the starting unit of representation are "words" (as we normally call it). Whether you want to create the document-"word" matrix to apply the algorithms based on word distribution (topic modelling, scaling) or you want to apply the fancier algorithms such as word2vector (which we will cover next week) or Transformers (the hot topic these days), you need to know what a "word" is, the main issue being that the definition must remain consistent within the task you do.

Thus, for any quantitative analysis, the text firstly needs to be normalized so that the definition of the unit ("word") you will be working with remains consistent. Normally, you first need to separate the text into "tokens", i.e., what we would traditionally imagine as "words". Most of the time, words are separated by a white space but not always (and you need to be able to handle these cases). The upper-lower case may pose a problem: is the word "whether" and "Whether" the same? Without specifying that the "Whether" needs to be read in lower case, the algorithms would treat them as different tokens. What constitutes a sentence when we want our unit of analysis to be sentences?

However, the most interesting problem (at least for our family of languages) is conjugation/declension. English rarely conjugates. However, German knows 4 cases, adjectives conjugate etc. Is the adjective "schick" the same for "ein schickes Fahrrad" and "eine schicke Brille"? Intuitively we would say yes, however, without accounting for the declension, the computer would treat both tokens as separate. The process of getting the "root" of a token (more technically the "lemma") is called *lemmatization* and it plays an important role in the processing of any text.

Luckily for us, there is a research team from Prague (yay!) that developed and trained models for a lot of languages, which can 

Typically, if we consider a "word" (or a token) as the unit of our analysis, we need to normalize the text by:
(1) removing any unnecessary whitespaces or punctuation,
(2) transforming all the text in lower case,
(3) lemmatizing the text,
(4) and removing stop words, words that do not typically carry any meaning (for the purpose we want to achieve, often including digits) and just bloat our corpus.

``` {r udpipe}
bvg_text_corpus = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/bvg_text_corpus.rds")) %>% 
  as_tibble() %>%
  slice_head(n = 100)

# Download the language model and determine the number of cores used for the process
n.cores = detectCores() - 2
ud_model = udpipe_download_model(language = "german")
ud_model = udpipe_load_model(ud_model$file_model)

# Parallesized
start = Sys.time()
bvg_udpipe = udpipe(x = bvg_text_corpus, object = ud_model, parallel.cores = n.cores) %>%
  as_tibble() %>% 
  select(c(doc_id, paragraph_id, start, end, token, lemma, upos))
  select()
  
end = Sys.time()

# Sequential
start = Sys.time()
bvg_udpipe = udpipe(x = bvg_text_corpus, object = ud_model, parallel.cores = 1) %>%
  as_tibble() %>%
  select(c(doc_id, paragraph_id, start, end, token, lemma, upos))
end = Sys.time()
```


## Onto fancier stuff - Machine Learning
Machine learning may be used for all of them - take input on data, learn to recognize the patterns and then apply them mostly to classify/scale/find similarity previously unseen data.

(1) Supervised - the researcher/developer pre-defines some characteristics/rules/labels of the training data - the recent "buzzwords" are support vector machines (SVM), random forests, gradient boosted trees... but also basic stuff such as regression.
(2) Unsupervised - the machine learning model tries to learn the patterns without assistance  - Latent Dirichlet Allocation (LDA) or Structured Topic Models (STM) for topic modelling, Wordfish for scaling speeches. 

There are also two types of models:
(1) Parametrized, which contain one or more hyperparameter(s) that needs to be determined before training the model on training data
(2) Non-parametrized models, which also contain parameters that are determined by the machine learning algorithm during the training process.

### Topic modeling and scaling
Topic modeling has started to play quite an important role in quantitative text analysis. In literary science, you can for example track the development of certain topics over time, in the social science topic modeling can be used to reduce bias in any inquiry (to ensure that you compare effect of something across similar type of texts).

The usual starting algorithm is Latent Dirichlet Allocation. It is based on the assumption that words are not equally distributed across topics (and so are the other topic modeling and scaling algorithms).

To this end, all these algorithms require that the data be processed in the form of a document-term/feature matrix. You can imagine it as a table, where one axis shows words [technically referred to as terms or features], the other documents (in our case court decisions), and the cells are filled out by the number of occurances of the word in the document. There are multiple ways to construct the dfm, we will try out the quanteda package and tidytext package

``` {r dfm}
# Creating a dtm/dfm multiple ways---

# Stop words - they are words that you think carry little to zero meaning in terms of distinguishing topics from each other
to_remove = tibble(
  word = c(
    "article", "court", "paragraph", "judgment", "case",
    "proceedings", "apeal", "application",
    "directive", "regulation", "law",
    "member", "state", "states", "commission", "european", "union", "eu", "eu:c"
  )
)

# Add the typical english stop words
to_remove = bind_rows(
  get_stopwords() %>% select(word),
  to_remove
)
# Quanteda
quanteda_corpus = text_corpus %>%
  select(ecli, text) %>% 
  group_by(ecli) %>%
  summarise(text = paste(text, collapse = " ")) %>%
  corpus(
  docid_field = "ecli",
  text_field = "text"
)

CJEU_dfm = tokens(quanteda_corpus, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE) %>% 
  tokens_select(pattern = to_remove$word, valuetype = "regex", selection = "remove", min_nchar=2L) %>%   dfm() %>% 
  dfm_trim(min_termfreq = 5)

# The tidy text way
# Create tidy text corpus
tidy_text_corpus = text_corpus %>%
  group_by(ecli) %>%
  unnest_tokens(
    output = "word",
    input = text,
    token = "words",
    to_lower = TRUE
  )

# Remove stop words, short words, numbers, and words with punctuation
tidy_text_corpus = tidy_text_corpus %>%
  anti_join(
    get_stopwords(),
    by = "word"
  ) %>%
  filter(
    !str_detect(word, "[0-9]")
  ) %>%
  filter(
    str_length(word) >= 2
  ) %>%
  filter(
    !str_detect(word, "[[:punct:]]")
  )

# Remove words
tidy_text_corpus = tidy_text_corpus %>%
  anti_join(
    to_remove,
    by = "word"
  )

# Frequency analysis
counts = tidy_text_corpus %>%
  group_by(word) %>%
  summarize(count = n()) %>%
  arrange(desc(count))

# Plot most frequent words
plot = tidy_text_corpus %>%
  group_by(word) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  slice_head(n = 25) %>%
  mutate(
    word = word %>%
      factor() %>%
      fct_reorder(count)
  ) %>%
  ggplot() +
  geom_bar(aes(x = word, y = count), stat = "identity", color = "black", fill = "gray90", width = 0.7) +
  scale_y_continuous(breaks = seq(0, 30000, 5000), expand = expansion(mult = c(0, 0.1))) +
  coord_flip() +
  labs(
    title = "Most frequent words in CJEU judgments (2019-2021)",
    x = NULL,
    y = "Frequency"
  ) +
  theme_minimal()

# Document feature matrix ------------------------------------------------------

# Create a dfm
CJEU_dfm_tidy = tidy_text_corpus %>%
  group_by(word, ecli) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) %>%
  cast_dfm(
    document = ecli,
    term = word,
    value = count
  )

# Check the dimensions
dim(CJEU_dfm_tidy)

# Trim the DF
CJEU_dfm_tidy_trimmed = CJEU_dfm_tidy %>%
  dfm_trim(min_termfreq = 5)

# Check the dimensions
dim(CJEU_dfm_tidy_trimmed)
dim(CJEU_dfm)
```

Now onto LDA. LDA is an example of so-called parametric models. Parametric models are such models that require an input of a parameter that somehow controls the learning process. There are two kinds:
(1) Hyperparameters that are tuned by the researcher and do not flow from the process of training on data.
(2) Parameters which the model learns during the process of training.

LDA contains one hyperparameter: the number of topics. 

```{r LDA}
# hyperparameter = ldatuning::FindTopicsNumber(
#   CJEU_dfm,
#   topics = seq(from = 14, to = 22, by = 2),
#   metrics = c("CaoJuan2009",  "Deveaud2014"),
#   method = "Gibbs",
#   control = list(seed = 77),
#   verbose = TRUE
# )
# 
# FindTopicsNumber_plot(hyperparameter)


# number of topics
K = 10

# set random number generator seed
set.seed(9161)

# compute the LDA model, inference via 1000 iterations of Gibbs sampling
cjeu_topic_model = textmodel_lda(CJEU_dfm_tidy_trimmed, K)
saveRDS(cjeu_topic_model, file = "../data/cjeu_topic_model.rds")
cjeu_topic_model = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/cjeu_topic_model.rds"))

# List 20 terms that define each topic
terms(cjeu_topic_model, n = 20)

# How the NSS dataset was generated
# Random prep for courts_and_judges
NSS_df = NSS_metadata %>%
  filter(type_decision == "Rozsudek") %>%
  mutate(outcome = case_when(
    grepl("zrušeno", type_verdict) ~ "granted",
    .default = "rejected"
  ),
  outcome = factor(outcome),
  phd = case_when(
    grepl("Ph.D.", lawyer) ~ TRUE,
    .default = FALSE)) %>%
  select(c(doc_id, lawyer, outcome, phd)) %>%
  group_by(phd) %>%
  mutate(ability = rnorm(n(), mean = phd + 4, sd = 1)) %>%
  ungroup() %>%
  drop_na()

# Check the ability distribution
NSS_df %>% 
  ggplot() +
  geom_density(aes(x = ability, group = phd, color = phd))
```

*Additional reading on topic modeling*
[Here](https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2) is a concise article explaining the basic intuition behind LDA.

Accoring to newer approaches (STM), LDA suffers from multiple issues:
(1) Topics within a document are independent of one another. In English: Just because document 1 has latent topic 1, it gives us no information whether document 1 has latent topic 2, 3, etc.
(2) The distribution of words within a topic (i.e. topic content) is stationary. In English: topic 1 for document 1 uses identical words as topic 1 for document 2, 3, etc.
(3)Topics can be modeled entirely based on the text of the document. In English: LDA only looks at the text of the document when determining topics, and doesn’t take any other information (author, date, source) into account.

The main problem for social science research, in my eyes, is the second topic. Thus a new topic modeling algorithm called Structural Topic Models has been developed, which attempts to overcome these issues. Namely, it uses known metadata about the documents to correct for the simple reliance on word distribution across topics. You can read more about [here](https://towardsdatascience.com/introduction-to-the-structural-topic-model-stm-34ec4bd5383).

## Scaling
Just a brief peek into the Wordfish model primarily developed for scaling political speeches. The code was copied from Joshua Fjelstul as I don't usually work with the Wordfish model

``` {r wordfish}
# Wordfish model ---------------------------------------------------------------

# Estimate model
wordfish_model = textmodel_wordfish(CJEU_dfm_tidy_trimmed)
saveRDS(wordfish_model, file = "../data/wordfish_model.rds")
wordfish_model = readRDS(url(""))

# Create the judge_rapporteurs df
judge_rapporteurs <- text_corpus %>%
  filter(str_detect(text, "^composed of")) %>%
  mutate(
    judge_rapporteur = text %>%
      str_extract("[A-Z](\\.)? [[:alpha:]' ]+(, President of the Chamber)?,? *\\([Rr]apporteure?\\)") %>%
      str_remove("\\([Rr]apporteure?\\)") %>%
      str_remove(", President of the Chamber") %>%
      str_remove("^[A-Z](\\.)?") %>%
      str_remove(",") %>%
      str_squish()
  )

# There are 4 parameters
# theta = estimated document positions
# beta = estimated word positions
# alpha = estimated document fixed effect (some documents are just longer than others)
# psi = estimated word fixed effect (some words are just more common)

## Interpret word estimates ----------------------------------------------------

# List of procedure-oriented words
procedural_words = c(
  "appeal", "appellant", "unsuccessful", "error", "plea", "pleas", "unfounded",
  "erred", "annulment", "infringement", "arguments", "alleging"
)

# List of policy-oriented words
policy_words = c(
  "worker", "workers", "social", "pension", "taxation", "income", "credit",
  "employ", "contract", "person", "work", "tax", "service", "insurance",
  "consumer", "employee", "citizen", "passenger", "passengers", "loan",
  "family", "child", "residence"
)

# Make a tibble with the word parameters
word_estimates = tibble(
  word = colnames(CJEU_dfm_tidy_trimmed),
  position = wordfish_model$beta,
  fixed_effect = wordfish_model$psi,
)

# Word type
word_estimates = word_estimates %>%
  mutate(
    word_type = case_when(
      word %in% procedural_words ~ "Procedural word",
      word %in% policy_words ~ "Policy word",
      TRUE ~ "Other"
    ),
    word_type = factor(
      word_type,
      levels = c("Policy word", "Procedural word", "Other")
    )
  )

# We can graph these to interpret the latent dimension
ggplot(word_estimates, aes(x = position, y = fixed_effect, label = word, color = word_type, size = word_type)) +
  geom_text(alpha = 0.7) +
  geom_vline(xintercept = 0, size = 0.5, linetype = "dashed") +
  scale_x_continuous(limits = c(-3, 3), breaks = seq(-3, 3, 1)) +
  scale_y_continuous(limits = c(-3, 3), breaks = seq(-3, 3, 1)) +
  scale_color_manual(values = c("#3498db", "#2ecc71", "gray80"), name = NULL) +
  scale_size_manual(values = c(4, 4, 2.5), guide = "none") +
  ggtitle("Wordfish estimates for the positions of words in CJEU judgments (2019-2021)") +
  ylab("Fixed effect") +
  xlab("Word position") +
  theme_minimal()

## Interpret document estimates ------------------------------------------------

# Create a table of document estimates
document_estimates = tibble(
  ecli = wordfish_model$docs,
  position = wordfish_model$theta
)

# Merge in judge rapporteur
document_estimates = document_estimates %>%
  left_join(
    judge_rapporteurs %>%
      select(
        ecli, judge_rapporteur
      ),
    by = "ecli"
  )

# Collapse by judge
judge_estimates = document_estimates %>%
  group_by(judge_rapporteur) %>%
  summarize(
    position = mean(position),
    count = n()
  ) %>%
  ungroup() %>%
  mutate(
    judge_rapporteur = judge_rapporteur %>%
      factor() %>%
      fct_reorder(position)
  ) %>%
  filter(
    count >= 10
  )

# Plot average positions by judge-rapporteur
ggplot(judge_estimates, aes(x = position, y = judge_rapporteur, size = count)) +
  geom_point(color = "#3498db") +
  geom_vline(xintercept = 0, size = 0.5, linetype = "dashed") +
  scale_size_continuous(range = c(1, 4), name = "Number of judgments") +
  ggtitle("Wordfish estimates for the positions CJEU judges (2019-2021)") +
  ylab(NULL) +
  xlab("Average judge-rapporteur position") +
  theme_minimal()

```

### Solutions


``` {r solutions}
# Regex
# Run the first
wiki = data.frame(wiki_text = c("Lee Epstein is a political scientist", "Arnold Schwarzenegger is an american actor", "D. Trump is a former US president"))

# And then run this and try to understand what just happened
wiki = wiki %>%
  mutate(
    name = wiki_text %>%
      str_extract(pattern = "[A-Z]\\.?[a-z]*\\s[A-Z][a-z]+")
  )

# Exercise 1
 judge_rapporteur = judge_rapporteur %>% 
  mutate(
    judge_rapporteur = text %>%
      str_extract("[A-Z]\\. [[:alpha:]]+ \\(Rapporteur\\)") %>%
      str_remove("\\([Rr]apporteur\\)") %>%
      str_remove("^[A-Z]\\.") %>%
      str_squish()
  )
```

