View(predict_75)
View(bike_model_df)
View(predict_75)
shortcut_prediction <-
posterior_predict(bike_model, newdata = data.frame(temp_feel = 75))
posterior_interval(shortcut_prediction, prob = 0.95)
?posterior_interval
mcmc_dens(shortcut_prediction) +
xlab("predicted ridership on a 75 degree day")
# Tune uncertain priors
bike_model_default <- stan_glm(
rides ~ temp_feel, data = bikes,
family = gaussian,
prior_intercept = normal(5000, 2.5, autoscale = TRUE),
prior = normal(0, 2.5, autoscale = TRUE),
prior_aux = exponential(1, autoscale = TRUE),
chains = 4, iter = 5000*2, seed = 84735)
prior_summary(bike_model_default)
bike_default_priors <- update(bike_model_default, prior_PD = TRUE)
?add_epred_draws
# Load and plot data
data(bikes)
# Try a regression yourself
# Prior information
plot_normal(mean = 5000, sd = 4000) +
labs(x = "interecept beta_0c", y = "pdf")
# Try a regression yourself
# Prior information
plot_normal(mean = 5000, sd = 1000) +
labs(x = "interecept beta_0c", y = "pdf")
# Try a regression yourself
# Prior information
plot_normal(mean = 5000, sd = 1500) +
labs(x = "interecept beta_0c", y = "pdf")
plot_normal(mean = 10, sd = 5) +
labs(x = "beta_1", y = "pdf")
plot_normal(mean = 10, sd = 3) +
labs(x = "beta_1", y = "pdf")
plot_gamma(shape = 2, rate = 1/2000) +
labs(x = "sigma", y = "pdf")
plot_gamma(shape = 3, rate = 1/2000) +
labs(x = "sigma", y = "pdf")
plot_gamma(shape = 1, rate = 1/2000) +
labs(x = "sigma", y = "pdf")
plot_gamma(shape = 1.5, rate = 1/2000) +
labs(x = "sigma", y = "pdf")
plot_gamma(shape = 1.2, rate = 1/2000) +
labs(x = "sigma", y = "pdf")
View(bikes)
View(bikes)
View(bikes)
bikes |>
ggplot(mapping = aes(x = temp_feel, y = rides)) +
geom_point() +
geom_smooth(method = "lm")
bikes |>
ggplot(mapping = aes(x = temp_feel, y = rides)) +
geom_point(size = 0.4) +
geom_smooth(method = "lm", sd = FALSE)
bikes |>
ggplot(mapping = aes(x = temp_feel, y = rides)) +
geom_point(size = 0.4) +
geom_smooth(method = "lm", se = FALSE)
View(bikes)
bikes |>
ggplot(mapping = aes(x = humidity, y = rides)) +
geom_point(size = 0.4) +
geom_smooth(method = "lm", se = FALSE)
?stan_glm
humidity_model = stan_glm(rides ~ humidity, data = bikes,
family = gaussian,
prior = normal(mean = 10, sd = 3),
prior_intercept = normal(mean = 5000, sd = 1500),
prior_aux = exponential(rate = 1/2000),
chains = 5, iter = 5000*2, seed = 84735)
humidity_model = stan_glm(rides ~ humidity, data = bikes,
family = gaussian,
prior = normal(10, 3),
prior_intercept = normal(5000, 1500),
prior_aux = exponential(1/2000),
chains = 5, iter = 5000*2, seed = 84735)
# Trace plots of parallel chains
mcmc_trace(humidity_model, size = 0.1)
# Density plots of parallel chains
mcmc_dens_overlay(humidity_model)
humidity_model |> tidy()
humidity_model |>
tidy(conf.int = TRUE, conf.level = 0.95)
?stan_glm
humidity_model = stan_glm(rides ~ humidity, data = bikes,
family = gaussian,
prior = normal(10, 3),
prior_intercept = normal(5000, 1500),
prior_aux = exponential(1/2000),
chains = 5, iter = 8000*2, seed = 84735,
prior_PD = TRUE)
humidity_model |>
tidy(conf.int = TRUE, conf.level = 0.95)
# This is only based on the prior understanding
humidity_model_prior = stan_glm(rides ~ humidity, data = bikes,
family = gaussian,
prior = normal(10, 3),
prior_intercept = normal(5000, 1500),
prior_aux = exponential(1/2000),
chains = 5, iter = 8000*2, seed = 84735,
prior_PD = TRUE)
# This includes the outcome
humidity_model = stan_glm(rides ~ humidity, data = bikes,
family = gaussian,
prior = normal(10, 3),
prior_intercept = normal(5000, 1500),
prior_aux = exponential(1/2000),
chains = 5, iter = 8000*2, seed = 84735,
prior_PD = TRUE)
humidity_model_prior |>
tidy(conf.int = TRUE, conf.level = 0.95)
humidity_model |>
tidy(conf.int = TRUE, conf.level = 0.95)
# This includes the outcome
humidity_model_posterior = stan_glm(rides ~ humidity, data = bikes,
family = gaussian,
prior = normal(10, 3),
prior_intercept = normal(5000, 1500),
prior_aux = exponential(1/2000),
chains = 5, iter = 8000*2, seed = 84735)
humidity_model_prior |>
tidy(conf.int = TRUE, conf.level = 0.95)
humidity_model |>
tidy(conf.int = TRUE, conf.level = 0.95)
humidity_model_posterior |>
tidy(conf.int = TRUE, conf.level = 0.95)
# Trace plots of parallel chains
mcmc_trace(humidity_model_posterior, size = 0.1)
# Density plots of parallel chains
mcmc_dens_overlay(humidity_model_posterior)
# Posterior model diagnostics
humidity_model_prior |>
tidy(effects = c("fixed", "aux"),
conf.int = TRUE, conf.level = 0.80)
humidity_model_posterior |>
tidy(effects = c("fixed", "aux"),
conf.int = TRUE, conf.level = 0.80)
bikes |>
add_epred_draws(humidity_model_posterior, ndraws = 50)
?add_epred_draws
humidity_model_df = humidity_model_posterior |>
as_tibble()
View(humidity_model_df)
bikes |>
add_epred_draws(humidity_model_posterior, ndraws = 50) |>
ggplot(mapping = aes(x = humidity, y = rides)) +
geom_point(size = 0.05)
epred = bikes |>
add_epred_draws(humidity_model_posterior, ndraws = 50)
View(epred)
bikes |>
add_epred_draws(humidity_model_posterior, ndraws = 50) |>
ggplot(mapping = aes(x = humidity, y = rides)) +
geom_point(size = 0.05) +
geom_line(aes(y = .epred, group = .draw), alpha = 0.15)
bikes |>
add_epred_draws(humidity_model_posterior, ndraws = 200) |>
ggplot(mapping = aes(x = humidity, y = rides)) +
geom_point(size = 0.05) +
geom_line(aes(y = .epred, group = .draw), alpha = 0.15)
humidity_model_posterior |>
tidy(effects = c("fixed", "aux"),
conf.int = TRUE, conf.level = 0.95)
View(humidity_model_df)
View(humidity_model_df)
View(bikes)
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(20000, mean = mu, sd = sigma))
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(20000, mean = avg_day, sd = sigma))
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(40000, mean = avg_day, sd = sigma))
View(humidity_bike_predict)
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(1, mean = avg_day, sd = sigma))
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(40000, mean = avg_day, sd = sigma))
humidity_bike_predict |>
ggplot(mapping = aes(x = avg_day)) +
geom_density()
humidity_bike_predict |>
ggplot(mapping = aes(x = avg_day)) +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow))
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow))
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow))
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow)) +
labs(x = "Riders")
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow)) +
labs(x = "riders")
humidity_bike_predict2 =
posterior_predict(humidity_model, newdata = data.frame(humidity = 90))
humidity_bike_predict2(shortcut_prediction)
humidity_bike_predict2 =
posterior_predict(humidity_model, newdata = data.frame(humidity = 90))
mcmc_dens(humidity_bike_predict2)
humidity_bike_predict2 =
posterior_predict(humidity_model, newdata = data.frame(humidity = 90))
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow)) +
labs(x = "riders")
mcmc_dens(humidity_bike_predict2)
posterior_interval(humidity_bike_predict2, prob = 0.95)
humidity_bike_predict |>
summarise(
lower_avg = quantile(avg_day, 0.025),
upper_avg = quantile(avg_day, 0.975),
lower_tmrw = quantile(tomorrow, 0.025),
upper_tmrw = quantile(tomorrow, 0.975)
)
View(humidity_model_df)
View(humidity_model_df)
View(humidity_bike_predict)
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(40000, mean = avg_day, sd = sigma))
humidity_bike_predict |>
summarise(
lower_avg = quantile(avg_day, 0.025),
upper_avg = quantile(avg_day, 0.975),
lower_tmrw = quantile(tomorrow, 0.025),
upper_tmrw = quantile(tomorrow, 0.975)
)
humidity_bike_predict2 =
posterior_predict(humidity_model, newdata = data.frame(humidity = 90))
posterior_interval(humidity_bike_predict2, prob = 0.95)
humidity_bike_predict |>
summarize(
lower_avg = quantile(avg_day, 0.025),
upper_avg = quantile(avg_day, 0.975),
lower_tmrw = quantile(tomorrow, 0.025),
upper_tmrw = quantile(tomorrow, 0.975)
)
View(humidity_model_df)
set.seed(84735)
humidity_bike_predict2 =
posterior_predict(humidity_model, newdata = data.frame(humidity = 90))
posterior_interval(humidity_bike_predict2, prob = 0.95)
# This includes the outcome - the posterior simulation
humidity_model_posterior = stan_glm(rides ~ humidity, data = bikes,
family = gaussian,
prior = normal(10, 3),
prior_intercept = normal(5000, 1500),
prior_aux = exponential(1/2000),
chains = 5, iter = 8000*2, seed = 84735)
humidity_model_df = humidity_model_posterior |>
as_tibble()
humidity_model_posterior |>
tidy(effects = c("fixed", "aux"),
conf.int = TRUE, conf.level = 0.95)
# Posterior model diagnostics
humidity_model_prior |>
tidy(effects = c("fixed", "aux"),
conf.int = TRUE, conf.level = 0.80)
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(40000, mean = avg_day, sd = sigma))
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow)) +
labs(x = "riders")
humidity_bike_predict |>
summarize(
lower_avg = quantile(avg_day, 0.025),
upper_avg = quantile(avg_day, 0.975),
lower_tmrw = quantile(tomorrow, 0.025),
upper_tmrw = quantile(tomorrow, 0.975)
)
humidity_bike_predict2 =
posterior_predict(humidity_model_posterior, newdata = data.frame(humidity = 90))
posterior_interval(humidity_bike_predict2, prob = 0.95)
set.seed(84735)
humidity_bike_predict2 =
posterior_predict(humidity_model_posterior, newdata = data.frame(humidity = 90))
posterior_interval(humidity_bike_predict2, prob = 0.95)
set.seed(84735)
humidity_bike_predict2 =
posterior_predict(humidity_model_posterior, newdata = data.frame(humidity = 90))
pp_check(humidity_model_posterior, nreps = 50) +
xlab("rides")
complete_prediction = posterior_predict(humidity_model_posterior, newdata = bikes)
dim(complete_prediction)
?ppc_intervals
ppc_intervals(y = bikes$rides, yrep = complete_prediction, x = bikes$humidity)
?ppc_intervals
ppc_intervals(y = bikes$rides, yrep = complete_prediction, x = bikes$humidity,
prob = 0.5, prob_outer = 0.95)
prediction_summary(model = humidity_model_prior, data = bikes)
set.seed(84735)
prediction_summary(model = humidity_model_prior, data = bikes)
# Creating a dtm/dfm multiple ways---
# Stop words - they are words that you think carry little to zero meaning in terms of distinguishing topics from each other
to_remove = tibble(
word = c(
"article", "court", "paragraph", "judgment", "case",
"proceedings", "apeal", "application",
"directive", "regulation", "law",
"member", "state", "states", "commission", "european", "union"
)
)
knitr::opts_chunk$set(eval = FALSE)
# Install packages
# install.packages("tidyverse")
# install.packages("lubridate")
# install.packages("tidytext")
# install.packages("quanteda")
# install.packages("quanteda.textstats")
# install.packages("quanteda.textmodels")
# install.packages("ldatuning")
# install.packages("seededlda")
# Load packages
library(tidyverse)
library(lubridate)
library(tidytext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)
library(ldatuning)
library(seededlda)
# Load data dynamically from a github page (should've done this earlier)
load(url("https://github.com/jfjelstul/law-and-courts-workshop/raw/master/data/text_corpus.RData")) %>% as_tibble()
bvg_text_corpus = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/bvg_text_corpus.rds")) %>% as_tibble()
# Creating a dtm/dfm multiple ways---
# Stop words - they are words that you think carry little to zero meaning in terms of distinguishing topics from each other
to_remove = tibble(
word = c(
"article", "court", "paragraph", "judgment", "case",
"proceedings", "apeal", "application",
"directive", "regulation", "law",
"member", "state", "states", "commission", "european", "union"
)
)
# Quanteda
quanteda_corpus = text_corpus %>%
select(ecli, text) %>%
group_by(ecli) %>%
summarise(text = paste(text, collapse = " ")) %>%
corpus(
docid_field = "ecli",
text_field = "text"
)
CJEU_dfm = tokens(quanteda_corpus, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE) %>%
tokens_select(pattern = to_remove$word, valuetype = "regex", selection = "remove", min_nchar=2L) %>%   dfm() %>%
dfm_trim(min_termfreq = 5)
# The tidy text way
# Create tidy text corpus
tidy_text_corpus = text_corpus %>%
group_by(ecli) %>%
unnest_tokens(
output = "word",
input = text,
token = "words",
to_lower = TRUE
)
# Remove stop words, short words, numbers, and words with punctuation
tidy_text_corpus = tidy_text_corpus %>%
anti_join(
get_stopwords(),
by = "word"
) %>%
filter(
!str_detect(word, "[0-9]")
) %>%
filter(
str_length(word) >= 2
) %>%
filter(
!str_detect(word, "[[:punct:]]")
)
# Remove words
tidy_text_corpus = tidy_text_corpus %>%
anti_join(
to_remove,
by = "word"
)
# Plot frequency by rank
# Zipf's law: A word's frequency is inversely proporational to its rank.
# The word at rank n appears 1/n times as often as the most frequent one.
plot <- tidy_text_corpus |>
group_by(word) |>
summarize(count = n()) |>
arrange(desc(count)) |>
ungroup() |>
mutate(
rank = row_number(),
frequency = count / sum(count)
) |>
ggplot() +
geom_line(aes(x = rank, y = frequency), size = 1, color = "#3498db") +
scale_x_log10() +
scale_y_log10() +
labs(
title = "Zipf's law for CJEU judgments",
x = "Rank",
y = "Frequency"
) +
theme_minimal()
# Document feature matrix ------------------------------------------------------
# Create a dfm
CJEU_dfm_tidy = tidy_text_corpus %>%
group_by(word, ecli) %>%
summarize(count = n()) %>%
arrange(desc(count)) %>%
cast_dfm(
document = ecli,
term = word,
value = count
)
# Check the dimensions
dim(CJEU_dfm_tidy)
# Trim the DF
CJEU_dfm_tidy_trimmed = CJEU_dfm_tidy %>%
dfm_trim(min_termfreq = 5)
# Check the dimensions
dim(CJEU_dfm_tidy_trimmed)
dim(CJEU_dfm)
# number of topics
K = 22
# set random number generator seed
set.seed(9161)
cjeu_topic_model = textmodel_lda(CJEU_dfm, K)
# Creating a dtm/dfm multiple ways---
# Stop words - they are words that you think carry little to zero meaning in terms of distinguishing topics from each other
to_remove = tibble(
word = c(
"article", "court", "paragraph", "judgment", "case",
"proceedings", "apeal", "application",
"directive", "regulation", "law",
"member", "state", "states", "commission", "european", "union"
)
)
knitr::opts_chunk$set(eval = FALSE)
# Install packages
# install.packages("tidyverse")
# install.packages("lubridate")
# install.packages("tidytext")
# install.packages("quanteda")
# install.packages("quanteda.textstats")
# install.packages("quanteda.textmodels")
# install.packages("ldatuning")
# install.packages("seededlda")
# Load packages
library(tidyverse)
library(lubridate)
library(tidytext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)
library(ldatuning)
library(seededlda)
# Load data dynamically from a github page (should've done this earlier)
load(url("https://github.com/jfjelstul/law-and-courts-workshop/raw/master/data/text_corpus.RData")) %>% as_tibble()
bvg_text_corpus = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/bvg_text_corpus.rds")) %>% as_tibble()
to_remove = tibble(
word = c(
"article", "court", "paragraph", "judgment", "case",
"proceedings", "apeal", "application",
"directive", "regulation", "law",
"member", "state", "states", "commission", "european", "union"
)
)
# Quanteda
quanteda_corpus = text_corpus %>%
select(ecli, text) %>%
group_by(ecli) %>%
summarise(text = paste(text, collapse = " ")) %>%
corpus(
docid_field = "ecli",
text_field = "text"
)
CJEU_dfm = tokens(quanteda_corpus, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE) %>%
tokens_select(pattern = to_remove$word, valuetype = "regex", selection = "remove", min_nchar=2L) %>%   dfm() %>%
dfm_trim(min_termfreq = 5)
