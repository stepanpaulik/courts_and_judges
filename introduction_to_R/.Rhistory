# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(1, mean = avg_day, sd = sigma))
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(40000, mean = avg_day, sd = sigma))
humidity_bike_predict |>
ggplot(mapping = aes(x = avg_day)) +
geom_density()
humidity_bike_predict |>
ggplot(mapping = aes(x = avg_day)) +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow))
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow))
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow))
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow)) +
labs(x = "Riders")
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow)) +
labs(x = "riders")
humidity_bike_predict2 =
posterior_predict(humidity_model, newdata = data.frame(humidity = 90))
humidity_bike_predict2(shortcut_prediction)
humidity_bike_predict2 =
posterior_predict(humidity_model, newdata = data.frame(humidity = 90))
mcmc_dens(humidity_bike_predict2)
humidity_bike_predict2 =
posterior_predict(humidity_model, newdata = data.frame(humidity = 90))
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow)) +
labs(x = "riders")
mcmc_dens(humidity_bike_predict2)
posterior_interval(humidity_bike_predict2, prob = 0.95)
humidity_bike_predict |>
summarise(
lower_avg = quantile(avg_day, 0.025),
upper_avg = quantile(avg_day, 0.975),
lower_tmrw = quantile(tomorrow, 0.025),
upper_tmrw = quantile(tomorrow, 0.975)
)
View(humidity_model_df)
View(humidity_model_df)
View(humidity_bike_predict)
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(40000, mean = avg_day, sd = sigma))
humidity_bike_predict |>
summarise(
lower_avg = quantile(avg_day, 0.025),
upper_avg = quantile(avg_day, 0.975),
lower_tmrw = quantile(tomorrow, 0.025),
upper_tmrw = quantile(tomorrow, 0.975)
)
humidity_bike_predict2 =
posterior_predict(humidity_model, newdata = data.frame(humidity = 90))
posterior_interval(humidity_bike_predict2, prob = 0.95)
humidity_bike_predict |>
summarize(
lower_avg = quantile(avg_day, 0.025),
upper_avg = quantile(avg_day, 0.975),
lower_tmrw = quantile(tomorrow, 0.025),
upper_tmrw = quantile(tomorrow, 0.975)
)
View(humidity_model_df)
set.seed(84735)
humidity_bike_predict2 =
posterior_predict(humidity_model, newdata = data.frame(humidity = 90))
posterior_interval(humidity_bike_predict2, prob = 0.95)
# This includes the outcome - the posterior simulation
humidity_model_posterior = stan_glm(rides ~ humidity, data = bikes,
family = gaussian,
prior = normal(10, 3),
prior_intercept = normal(5000, 1500),
prior_aux = exponential(1/2000),
chains = 5, iter = 8000*2, seed = 84735)
humidity_model_df = humidity_model_posterior |>
as_tibble()
humidity_model_posterior |>
tidy(effects = c("fixed", "aux"),
conf.int = TRUE, conf.level = 0.95)
# Posterior model diagnostics
humidity_model_prior |>
tidy(effects = c("fixed", "aux"),
conf.int = TRUE, conf.level = 0.80)
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(40000, mean = avg_day, sd = sigma))
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow)) +
labs(x = "riders")
humidity_bike_predict |>
summarize(
lower_avg = quantile(avg_day, 0.025),
upper_avg = quantile(avg_day, 0.975),
lower_tmrw = quantile(tomorrow, 0.025),
upper_tmrw = quantile(tomorrow, 0.975)
)
humidity_bike_predict2 =
posterior_predict(humidity_model_posterior, newdata = data.frame(humidity = 90))
posterior_interval(humidity_bike_predict2, prob = 0.95)
set.seed(84735)
humidity_bike_predict2 =
posterior_predict(humidity_model_posterior, newdata = data.frame(humidity = 90))
posterior_interval(humidity_bike_predict2, prob = 0.95)
set.seed(84735)
humidity_bike_predict2 =
posterior_predict(humidity_model_posterior, newdata = data.frame(humidity = 90))
pp_check(humidity_model_posterior, nreps = 50) +
xlab("rides")
complete_prediction = posterior_predict(humidity_model_posterior, newdata = bikes)
dim(complete_prediction)
?ppc_intervals
ppc_intervals(y = bikes$rides, yrep = complete_prediction, x = bikes$humidity)
?ppc_intervals
ppc_intervals(y = bikes$rides, yrep = complete_prediction, x = bikes$humidity,
prob = 0.5, prob_outer = 0.95)
prediction_summary(model = humidity_model_prior, data = bikes)
set.seed(84735)
prediction_summary(model = humidity_model_prior, data = bikes)
knitr::opts_chunk$set(eval = FALSE)
# Install packages
# install.packages("tidyverse")
# install.packages("lubridate")
# install.packages("tidytext")
# install.packages("quanteda")
# install.packages("quanteda.textstats")
# install.packages("quanteda.textmodels")
# install.packages("ldatuning")
# install.packages("seededlda")
# Load packages
library(tidyverse)
library(lubridate)
library(tidytext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)
library(ldatuning)
library(seededlda)
# Load data dynamically from a github page (should've done this earlier)
load(url("https://github.com/jfjelstul/law-and-courts-workshop/raw/master/data/text_corpus.RData")) %>% as_tibble()
bvg_text_corpus = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/bvg_text_corpus.rds")) %>% as_tibble()
# Creating a dtm/dfm multiple ways---
# Stop words - they are words that you think carry little to zero meaning in terms of distinguishing topics from each other
to_remove = tibble(
word = c(
"article", "court", "paragraph", "judgment", "case",
"proceedings", "apeal", "application",
"directive", "regulation", "law",
"member", "state", "states", "commission", "european", "union"
)
)
# Add the typical english stop words
to_remove = bind_rows(
get_stopwords() %>% select(word),
to_remove
)
# Quanteda
quanteda_corpus = text_corpus %>%
select(ecli, text) %>%
group_by(ecli) %>%
summarise(text = paste(text, collapse = " ")) %>%
corpus(
docid_field = "ecli",
text_field = "text"
)
CJEU_dfm = tokens(quanteda_corpus, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE) %>%
tokens_select(pattern = to_remove$word, valuetype = "regex", selection = "remove", min_nchar=2L) %>%   dfm() %>%
dfm_trim(min_termfreq = 5)
# number of topics
K = 10
cjeu_topic_model = textmodel_lda(CJEU_dfm, K)
saveRDS(cjeu_topic_model, file = "../data/cjeu_topic_model.rds")
# List 20 terms that define each topic
terms(cjeu_topic_model, n = 20)
# Stop words - they are words that you think carry little to zero meaning in terms of distinguishing topics from each other
to_remove = tibble(
word = c(
"article", "court", "paragraph", "judgment", "case",
"proceedings", "apeal", "application",
"directive", "regulation", "law",
"member", "state", "states", "commission", "european", "union", "eu", "eu:c"
)
)
to_remove = tibble(
word = c(
"article", "court", "paragraph", "judgment", "case",
"proceedings", "apeal", "application",
"directive", "regulation", "law",
"member", "state", "states", "commission", "european", "union", "eu", "eu:c"
)
)
# Add the typical english stop words
to_remove = bind_rows(
get_stopwords() %>% select(word),
to_remove
)
# Quanteda
quanteda_corpus = text_corpus %>%
select(ecli, text) %>%
group_by(ecli) %>%
summarise(text = paste(text, collapse = " ")) %>%
corpus(
docid_field = "ecli",
text_field = "text"
)
CJEU_dfm = tokens(quanteda_corpus, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE) %>%
tokens_select(pattern = to_remove$word, valuetype = "regex", selection = "remove", min_nchar=2L) %>%   dfm() %>%
dfm_trim(min_termfreq = 5)
# number of topics
K = 10
cjeu_topic_model = textmodel_lda(CJEU_dfm, K)
# List 20 terms that define each topic
terms(cjeu_topic_model, n = 20)
saveRDS(cjeu_topic_model, file = "../data/cjeu_topic_model.rds")
length(unique(text_corpus$ecli))
# List 20 terms that define each topic
terms(cjeu_topic_model, n = 20)
# List 20 terms that define each topic
terms(cjeu_topic_model, n = 20)
saveRDS(cjeu_topic_model, file = "../data/cjeu_topic_model.rds")
cjeu_topic_model = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/cjeu_topic_model.rds"))
CJEU_dfm
# Load packages
library(parallel)
library(tidyverse)
library(lubridate)
library(tidytext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)
library(ldatuning)
library(seededlda)
library(udpipe)
# Stop words - they are words that you think carry little to zero meaning in terms of distinguishing topics from each other
to_remove = tibble(
word = c(
"article", "court", "paragraph", "judgment", "case",
"proceedings", "apeal", "application",
"directive", "regulation", "law",
"member", "state", "states", "commission", "european", "union", "eu", "eu:c"
)
)
# Add the typical english stop words
to_remove = bind_rows(
get_stopwords() %>% select(word),
to_remove
)
tidy_text_corpus = text_corpus %>%
group_by(ecli) %>%
unnest_tokens(
output = "word",
input = text,
token = "words",
to_lower = TRUE
)
# Load data dynamically from a github page (should've done this earlier)
load(url("https://github.com/jfjelstul/law-and-courts-workshop/raw/master/data/text_corpus.RData")) %>% as_tibble()
tidy_text_corpus = text_corpus %>%
group_by(ecli) %>%
unnest_tokens(
output = "word",
input = text,
token = "words",
to_lower = TRUE
)
# Remove stop words, short words, numbers, and words with punctuation
tidy_text_corpus = tidy_text_corpus %>%
anti_join(
get_stopwords(),
by = "word"
) %>%
filter(
!str_detect(word, "[0-9]")
) %>%
filter(
str_length(word) >= 2
) %>%
filter(
!str_detect(word, "[[:punct:]]")
)
# Remove words
tidy_text_corpus = tidy_text_corpus %>%
anti_join(
to_remove,
by = "word"
)
# Plot frequency by rank
# Zipf's law: A word's frequency is inversely proporational to its rank.
# The word at rank n appears 1/n times as often as the most frequent one.
plot <- tidy_text_corpus |>
group_by(word) |>
summarize(count = n()) |>
arrange(desc(count)) |>
ungroup() |>
mutate(
rank = row_number(),
frequency = count / sum(count)
) |>
ggplot() +
geom_line(aes(x = rank, y = frequency), size = 1, color = "#3498db") +
scale_x_log10() +
scale_y_log10() +
labs(
title = "Zipf's law for CJEU judgments",
x = "Rank",
y = "Frequency"
) +
theme_minimal()
# Plot frequency by rank
# Zipf's law: A word's frequency is inversely proporational to its rank.
# The word at rank n appears 1/n times as often as the most frequent one.
tidy_text_corpus |>
group_by(word) |>
summarize(count = n()) |>
arrange(desc(count)) |>
ungroup() |>
mutate(
rank = row_number(),
frequency = count / sum(count)
) |>
ggplot() +
geom_line(aes(x = rank, y = frequency), size = 1, color = "#3498db") +
scale_x_log10() +
scale_y_log10() +
labs(
title = "Zipf's law for CJEU judgments",
x = "Rank",
y = "Frequency"
) +
theme_minimal()
# Create a dfm
CJEU_dfm_tidy = tidy_text_corpus %>%
group_by(word, ecli) %>%
summarize(count = n()) %>%
arrange(desc(count)) %>%
cast_dfm(
document = ecli,
term = word,
value = count
)
# Check the dimensions
dim(CJEU_dfm_tidy)
# Trim the DF
CJEU_dfm_tidy_trimmed = CJEU_dfm_tidy %>%
dfm_trim(min_termfreq = 5)
# number of topics
K = 10
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
cjeu_topic_model = textmodel_lda(CJEU_dfm_tidy_trimmed, K)
saveRDS(cjeu_topic_model, file = "../data/cjeu_topic_model.rds")
setwd("~/Library/CloudStorage/OneDrive-Humboldt-UniversitaetzuBerlin,CMS/PhD/Courts and Judges/courts_and_judges/introduction_to_R")
saveRDS(cjeu_topic_model, file = "../data/cjeu_topic_model.rds")
# List 20 terms that define each topic
terms(cjeu_topic_model, n = 20)
knitr::opts_chunk$set(eval = FALSE)
library(skimr)
library(tidymodels)
library(tidyverse)
library(themis)
# Load data
data_exercise = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/annotations_without_dissents.rds"))
library(skimr)
library(tidymodels)
library(tidyverse)
library(themis)
library(skimr)
library(tidymodels)
library(tidyverse)
library(themis)
data = load(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/train_test_ml.RData"))
data = load(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/train_test_ml.RData"))
data = rbind(test_data, train_data)
setwd("~/Library/CloudStorage/OneDrive-Humboldt-UniversitaetzuBerlin,CMS/PhD/Courts and Judges/courts_and_judges/introduction_to_R")
saveRDS(data, "../data/rand_forest_exercise.rds")
View(test_data)
View(test_data)
?all_double
rec = recipe(class ~ ., data = train_data) %>%
update_role(iuropa_decision_id, new_role = "ID") %>% # Here determine which of the columns is the ID
update_role(succesfull, new_role = "outcome") %>% # Determine here which of the columns is the outcome
update_role(all_predictos(), new_role = "predictor")
rec = recipe(class ~ ., data = train_data) %>%
update_role(iuropa_decision_id, new_role = "ID") %>% # Here determine which of the columns is the ID
update_role(succesfull, new_role = "outcome") %>% # Determine here which of the columns is the outcome
update_role(all_predictos(), new_role = "predictor")
rec = recipe(succesful ~ ., data = train_data) %>%
update_role(iuropa_decision_id, new_role = "ID") %>% # Here determine which of the columns is the ID
update_role(succesful, new_role = "outcome") %>% # Determine here which of the columns is the outcome
update_role(all_predictos(), new_role = "predictor")
rec = recipe(successful ~ ., data = train_data) %>%
update_role(iuropa_decision_id, new_role = "ID") %>% # Here determine which of the columns is the ID
update_role(successful, new_role = "outcome") %>% # Determine here which of the columns is the outcome
update_role(all_predictos(), new_role = "predictor")
rec = recipe(successful ~ ., data = train_data) %>%
update_role(iuropa_decision_id, new_role = "ID") %>% # Here determine which of the columns is the ID
update_role(successful, new_role = "outcome") %>% # Determine here which of the columns is the outcome
update_role(all_predictors(), new_role = "predictor")
rec
view(data)
model = rand_forest(
mtry = tune(),
trees = 1000,
min_n = tune()
) %>%
set_mode("classification") %>%
set_engine("ranger")
mod_tune = rand_forest(
mtry = tune(),
trees = 1000,
min_n = tune()
) %>%
set_mode("classification") %>%
set_engine("ranger")
wfl_tune = workflow() %>%
add_recipe(rec) %>%
add_model(mod_tune)
# Prepare data for k-fold cross-validation
folds = vfold_cv(train_data, v = 6)
fit_tune = tune_grid(
wfl_tune,
resamples = folds,
grid = 20
)
install.packages("ranger")
fit_tune = tune_grid(
wfl_tune,
resamples = folds,
grid = 20
)
View(data)
source("~/.active-rstudio-document", echo=TRUE)
# Plot the tuning result
fit_tune %>%
collect_metrics() %>%
filter(.metric == "roc_auc") %>%
select(mean, min_n, mtry) %>%
pivot_longer(min_n:mtry,
values_to = "value",
names_to = "parameter"
) %>%
ggplot(aes(value, mean, color = parameter)) +
geom_point(show.legend = FALSE) +
facet_wrap(~parameter, scales = "free_x") +
labs(x = NULL, y = "AUC")
data = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/rand_forest_exercise.rds")) %>% drop_na()
data = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/rand_forest_exercise.rds")) %>% drop_na()
library(skimr)
library(tidymodels)
library(tidyverse)
data = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/rand_forest_exercise.rds")) %>% drop_na()
data = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/rand_forest_exercise.rds")) %>% drop_na()
data = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/rand_forest_exercise.rds")) %>% drop_na()
# Familiarise yourself with the data
skim(data)
data = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/rand_forest_exercise.rds")) %>% drop_na()
View(data)
data = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/rand_forest_exercise.rds"))
data = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/rand_forest_exercise.rds"))
data = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/rand_forest_exercise.rds")) %>% drop_na()
library(skimr)
library(tidymodels)
library(tidyverse)
# Split the data into testing and training data
data_split = initial_split(data, prop = 3/4)
train_data = training(data_split)
test_data  = testing(data_split)
# Prepare data for k-fold cross-validation
folds = vfold_cv(train_data, v = 6)
View(data)
rec = recipe(successful ~ ., data = train_data) %>%
update_role(iuropa_decision_id, new_role = "ID") %>%
update_role(successful, new_role = "outcome") %>%
update_role(all_predictors(), new_role = "predictor")
rec
mod_tune = rand_forest(
mtry = tune(),
trees = 1000,
min_n = tune()
) %>%
set_mode("classification") %>%
set_engine("ranger")
wfl_tune = workflow() %>%
add_recipe(rec) %>%
add_model(mod_tune)
fit_tune = tune_grid(
wfl_tune,
resamples = folds,
grid = 20
)
