ggplot(mapping = aes(x = temp_feel, y = rides)) +
geom_point() +
geom_smooth(method = "lm")
bikes |>
ggplot(mapping = aes(x = temp_feel, y = rides)) +
geom_point(size = 0.4) +
geom_smooth(method = "lm", sd = FALSE)
bikes |>
ggplot(mapping = aes(x = temp_feel, y = rides)) +
geom_point(size = 0.4) +
geom_smooth(method = "lm", se = FALSE)
View(bikes)
bikes |>
ggplot(mapping = aes(x = humidity, y = rides)) +
geom_point(size = 0.4) +
geom_smooth(method = "lm", se = FALSE)
?stan_glm
humidity_model = stan_glm(rides ~ humidity, data = bikes,
family = gaussian,
prior = normal(mean = 10, sd = 3),
prior_intercept = normal(mean = 5000, sd = 1500),
prior_aux = exponential(rate = 1/2000),
chains = 5, iter = 5000*2, seed = 84735)
humidity_model = stan_glm(rides ~ humidity, data = bikes,
family = gaussian,
prior = normal(10, 3),
prior_intercept = normal(5000, 1500),
prior_aux = exponential(1/2000),
chains = 5, iter = 5000*2, seed = 84735)
# Trace plots of parallel chains
mcmc_trace(humidity_model, size = 0.1)
# Density plots of parallel chains
mcmc_dens_overlay(humidity_model)
humidity_model |> tidy()
humidity_model |>
tidy(conf.int = TRUE, conf.level = 0.95)
?stan_glm
humidity_model = stan_glm(rides ~ humidity, data = bikes,
family = gaussian,
prior = normal(10, 3),
prior_intercept = normal(5000, 1500),
prior_aux = exponential(1/2000),
chains = 5, iter = 8000*2, seed = 84735,
prior_PD = TRUE)
humidity_model |>
tidy(conf.int = TRUE, conf.level = 0.95)
# This is only based on the prior understanding
humidity_model_prior = stan_glm(rides ~ humidity, data = bikes,
family = gaussian,
prior = normal(10, 3),
prior_intercept = normal(5000, 1500),
prior_aux = exponential(1/2000),
chains = 5, iter = 8000*2, seed = 84735,
prior_PD = TRUE)
# This includes the outcome
humidity_model = stan_glm(rides ~ humidity, data = bikes,
family = gaussian,
prior = normal(10, 3),
prior_intercept = normal(5000, 1500),
prior_aux = exponential(1/2000),
chains = 5, iter = 8000*2, seed = 84735,
prior_PD = TRUE)
humidity_model_prior |>
tidy(conf.int = TRUE, conf.level = 0.95)
humidity_model |>
tidy(conf.int = TRUE, conf.level = 0.95)
# This includes the outcome
humidity_model_posterior = stan_glm(rides ~ humidity, data = bikes,
family = gaussian,
prior = normal(10, 3),
prior_intercept = normal(5000, 1500),
prior_aux = exponential(1/2000),
chains = 5, iter = 8000*2, seed = 84735)
humidity_model_prior |>
tidy(conf.int = TRUE, conf.level = 0.95)
humidity_model |>
tidy(conf.int = TRUE, conf.level = 0.95)
humidity_model_posterior |>
tidy(conf.int = TRUE, conf.level = 0.95)
# Trace plots of parallel chains
mcmc_trace(humidity_model_posterior, size = 0.1)
# Density plots of parallel chains
mcmc_dens_overlay(humidity_model_posterior)
# Posterior model diagnostics
humidity_model_prior |>
tidy(effects = c("fixed", "aux"),
conf.int = TRUE, conf.level = 0.80)
humidity_model_posterior |>
tidy(effects = c("fixed", "aux"),
conf.int = TRUE, conf.level = 0.80)
bikes |>
add_epred_draws(humidity_model_posterior, ndraws = 50)
?add_epred_draws
humidity_model_df = humidity_model_posterior |>
as_tibble()
View(humidity_model_df)
bikes |>
add_epred_draws(humidity_model_posterior, ndraws = 50) |>
ggplot(mapping = aes(x = humidity, y = rides)) +
geom_point(size = 0.05)
epred = bikes |>
add_epred_draws(humidity_model_posterior, ndraws = 50)
View(epred)
bikes |>
add_epred_draws(humidity_model_posterior, ndraws = 50) |>
ggplot(mapping = aes(x = humidity, y = rides)) +
geom_point(size = 0.05) +
geom_line(aes(y = .epred, group = .draw), alpha = 0.15)
bikes |>
add_epred_draws(humidity_model_posterior, ndraws = 200) |>
ggplot(mapping = aes(x = humidity, y = rides)) +
geom_point(size = 0.05) +
geom_line(aes(y = .epred, group = .draw), alpha = 0.15)
humidity_model_posterior |>
tidy(effects = c("fixed", "aux"),
conf.int = TRUE, conf.level = 0.95)
View(humidity_model_df)
View(humidity_model_df)
View(bikes)
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(20000, mean = mu, sd = sigma))
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(20000, mean = avg_day, sd = sigma))
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(40000, mean = avg_day, sd = sigma))
View(humidity_bike_predict)
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(1, mean = avg_day, sd = sigma))
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(40000, mean = avg_day, sd = sigma))
humidity_bike_predict |>
ggplot(mapping = aes(x = avg_day)) +
geom_density()
humidity_bike_predict |>
ggplot(mapping = aes(x = avg_day)) +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow))
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow))
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow))
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow)) +
labs(x = "Riders")
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow)) +
labs(x = "riders")
humidity_bike_predict2 =
posterior_predict(humidity_model, newdata = data.frame(humidity = 90))
humidity_bike_predict2(shortcut_prediction)
humidity_bike_predict2 =
posterior_predict(humidity_model, newdata = data.frame(humidity = 90))
mcmc_dens(humidity_bike_predict2)
humidity_bike_predict2 =
posterior_predict(humidity_model, newdata = data.frame(humidity = 90))
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow)) +
labs(x = "riders")
mcmc_dens(humidity_bike_predict2)
posterior_interval(humidity_bike_predict2, prob = 0.95)
humidity_bike_predict |>
summarise(
lower_avg = quantile(avg_day, 0.025),
upper_avg = quantile(avg_day, 0.975),
lower_tmrw = quantile(tomorrow, 0.025),
upper_tmrw = quantile(tomorrow, 0.975)
)
View(humidity_model_df)
View(humidity_model_df)
View(humidity_bike_predict)
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(40000, mean = avg_day, sd = sigma))
humidity_bike_predict |>
summarise(
lower_avg = quantile(avg_day, 0.025),
upper_avg = quantile(avg_day, 0.975),
lower_tmrw = quantile(tomorrow, 0.025),
upper_tmrw = quantile(tomorrow, 0.975)
)
humidity_bike_predict2 =
posterior_predict(humidity_model, newdata = data.frame(humidity = 90))
posterior_interval(humidity_bike_predict2, prob = 0.95)
humidity_bike_predict |>
summarize(
lower_avg = quantile(avg_day, 0.025),
upper_avg = quantile(avg_day, 0.975),
lower_tmrw = quantile(tomorrow, 0.025),
upper_tmrw = quantile(tomorrow, 0.975)
)
View(humidity_model_df)
set.seed(84735)
humidity_bike_predict2 =
posterior_predict(humidity_model, newdata = data.frame(humidity = 90))
posterior_interval(humidity_bike_predict2, prob = 0.95)
# This includes the outcome - the posterior simulation
humidity_model_posterior = stan_glm(rides ~ humidity, data = bikes,
family = gaussian,
prior = normal(10, 3),
prior_intercept = normal(5000, 1500),
prior_aux = exponential(1/2000),
chains = 5, iter = 8000*2, seed = 84735)
humidity_model_df = humidity_model_posterior |>
as_tibble()
humidity_model_posterior |>
tidy(effects = c("fixed", "aux"),
conf.int = TRUE, conf.level = 0.95)
# Posterior model diagnostics
humidity_model_prior |>
tidy(effects = c("fixed", "aux"),
conf.int = TRUE, conf.level = 0.80)
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(40000, mean = avg_day, sd = sigma))
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow)) +
labs(x = "riders")
humidity_bike_predict |>
summarize(
lower_avg = quantile(avg_day, 0.025),
upper_avg = quantile(avg_day, 0.975),
lower_tmrw = quantile(tomorrow, 0.025),
upper_tmrw = quantile(tomorrow, 0.975)
)
humidity_bike_predict2 =
posterior_predict(humidity_model_posterior, newdata = data.frame(humidity = 90))
posterior_interval(humidity_bike_predict2, prob = 0.95)
set.seed(84735)
humidity_bike_predict2 =
posterior_predict(humidity_model_posterior, newdata = data.frame(humidity = 90))
posterior_interval(humidity_bike_predict2, prob = 0.95)
set.seed(84735)
humidity_bike_predict2 =
posterior_predict(humidity_model_posterior, newdata = data.frame(humidity = 90))
pp_check(humidity_model_posterior, nreps = 50) +
xlab("rides")
complete_prediction = posterior_predict(humidity_model_posterior, newdata = bikes)
dim(complete_prediction)
?ppc_intervals
ppc_intervals(y = bikes$rides, yrep = complete_prediction, x = bikes$humidity)
?ppc_intervals
ppc_intervals(y = bikes$rides, yrep = complete_prediction, x = bikes$humidity,
prob = 0.5, prob_outer = 0.95)
prediction_summary(model = humidity_model_prior, data = bikes)
set.seed(84735)
prediction_summary(model = humidity_model_prior, data = bikes)
knitr::opts_chunk$set(eval = FALSE)
# Install packages
# install.packages("tidyverse")
# install.packages("lubridate")
# install.packages("tidytext")
# install.packages("quanteda")
# install.packages("quanteda.textstats")
# install.packages("quanteda.textmodels")
# install.packages("ldatuning")
# install.packages("seededlda")
# Load packages
library(tidyverse)
library(lubridate)
library(tidytext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)
library(ldatuning)
library(seededlda)
# Load data dynamically from a github page (should've done this earlier)
load(url("https://github.com/jfjelstul/law-and-courts-workshop/raw/master/data/text_corpus.RData")) %>% as_tibble()
bvg_text_corpus = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/bvg_text_corpus.rds")) %>% as_tibble()
# Creating a dtm/dfm multiple ways---
# Stop words - they are words that you think carry little to zero meaning in terms of distinguishing topics from each other
to_remove = tibble(
word = c(
"article", "court", "paragraph", "judgment", "case",
"proceedings", "apeal", "application",
"directive", "regulation", "law",
"member", "state", "states", "commission", "european", "union"
)
)
# Add the typical english stop words
to_remove = bind_rows(
get_stopwords() %>% select(word),
to_remove
)
# Quanteda
quanteda_corpus = text_corpus %>%
select(ecli, text) %>%
group_by(ecli) %>%
summarise(text = paste(text, collapse = " ")) %>%
corpus(
docid_field = "ecli",
text_field = "text"
)
CJEU_dfm = tokens(quanteda_corpus, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE) %>%
tokens_select(pattern = to_remove$word, valuetype = "regex", selection = "remove", min_nchar=2L) %>%   dfm() %>%
dfm_trim(min_termfreq = 5)
# number of topics
K = 10
cjeu_topic_model = textmodel_lda(CJEU_dfm, K)
saveRDS(cjeu_topic_model, file = "../data/cjeu_topic_model.rds")
# List 20 terms that define each topic
terms(cjeu_topic_model, n = 20)
# Stop words - they are words that you think carry little to zero meaning in terms of distinguishing topics from each other
to_remove = tibble(
word = c(
"article", "court", "paragraph", "judgment", "case",
"proceedings", "apeal", "application",
"directive", "regulation", "law",
"member", "state", "states", "commission", "european", "union", "eu", "eu:c"
)
)
to_remove = tibble(
word = c(
"article", "court", "paragraph", "judgment", "case",
"proceedings", "apeal", "application",
"directive", "regulation", "law",
"member", "state", "states", "commission", "european", "union", "eu", "eu:c"
)
)
# Add the typical english stop words
to_remove = bind_rows(
get_stopwords() %>% select(word),
to_remove
)
# Quanteda
quanteda_corpus = text_corpus %>%
select(ecli, text) %>%
group_by(ecli) %>%
summarise(text = paste(text, collapse = " ")) %>%
corpus(
docid_field = "ecli",
text_field = "text"
)
CJEU_dfm = tokens(quanteda_corpus, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE) %>%
tokens_select(pattern = to_remove$word, valuetype = "regex", selection = "remove", min_nchar=2L) %>%   dfm() %>%
dfm_trim(min_termfreq = 5)
# number of topics
K = 10
cjeu_topic_model = textmodel_lda(CJEU_dfm, K)
# List 20 terms that define each topic
terms(cjeu_topic_model, n = 20)
saveRDS(cjeu_topic_model, file = "../data/cjeu_topic_model.rds")
length(unique(text_corpus$ecli))
# List 20 terms that define each topic
terms(cjeu_topic_model, n = 20)
# List 20 terms that define each topic
terms(cjeu_topic_model, n = 20)
saveRDS(cjeu_topic_model, file = "../data/cjeu_topic_model.rds")
cjeu_topic_model = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/cjeu_topic_model.rds"))
CJEU_dfm
# Load packages
library(parallel)
library(tidyverse)
library(lubridate)
library(tidytext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)
library(ldatuning)
library(seededlda)
library(udpipe)
# Stop words - they are words that you think carry little to zero meaning in terms of distinguishing topics from each other
to_remove = tibble(
word = c(
"article", "court", "paragraph", "judgment", "case",
"proceedings", "apeal", "application",
"directive", "regulation", "law",
"member", "state", "states", "commission", "european", "union", "eu", "eu:c"
)
)
# Add the typical english stop words
to_remove = bind_rows(
get_stopwords() %>% select(word),
to_remove
)
tidy_text_corpus = text_corpus %>%
group_by(ecli) %>%
unnest_tokens(
output = "word",
input = text,
token = "words",
to_lower = TRUE
)
# Load data dynamically from a github page (should've done this earlier)
load(url("https://github.com/jfjelstul/law-and-courts-workshop/raw/master/data/text_corpus.RData")) %>% as_tibble()
tidy_text_corpus = text_corpus %>%
group_by(ecli) %>%
unnest_tokens(
output = "word",
input = text,
token = "words",
to_lower = TRUE
)
# Remove stop words, short words, numbers, and words with punctuation
tidy_text_corpus = tidy_text_corpus %>%
anti_join(
get_stopwords(),
by = "word"
) %>%
filter(
!str_detect(word, "[0-9]")
) %>%
filter(
str_length(word) >= 2
) %>%
filter(
!str_detect(word, "[[:punct:]]")
)
# Remove words
tidy_text_corpus = tidy_text_corpus %>%
anti_join(
to_remove,
by = "word"
)
# Plot frequency by rank
# Zipf's law: A word's frequency is inversely proporational to its rank.
# The word at rank n appears 1/n times as often as the most frequent one.
plot <- tidy_text_corpus |>
group_by(word) |>
summarize(count = n()) |>
arrange(desc(count)) |>
ungroup() |>
mutate(
rank = row_number(),
frequency = count / sum(count)
) |>
ggplot() +
geom_line(aes(x = rank, y = frequency), size = 1, color = "#3498db") +
scale_x_log10() +
scale_y_log10() +
labs(
title = "Zipf's law for CJEU judgments",
x = "Rank",
y = "Frequency"
) +
theme_minimal()
# Plot frequency by rank
# Zipf's law: A word's frequency is inversely proporational to its rank.
# The word at rank n appears 1/n times as often as the most frequent one.
tidy_text_corpus |>
group_by(word) |>
summarize(count = n()) |>
arrange(desc(count)) |>
ungroup() |>
mutate(
rank = row_number(),
frequency = count / sum(count)
) |>
ggplot() +
geom_line(aes(x = rank, y = frequency), size = 1, color = "#3498db") +
scale_x_log10() +
scale_y_log10() +
labs(
title = "Zipf's law for CJEU judgments",
x = "Rank",
y = "Frequency"
) +
theme_minimal()
# Create a dfm
CJEU_dfm_tidy = tidy_text_corpus %>%
group_by(word, ecli) %>%
summarize(count = n()) %>%
arrange(desc(count)) %>%
cast_dfm(
document = ecli,
term = word,
value = count
)
# Check the dimensions
dim(CJEU_dfm_tidy)
# Trim the DF
CJEU_dfm_tidy_trimmed = CJEU_dfm_tidy %>%
dfm_trim(min_termfreq = 5)
# number of topics
K = 10
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
cjeu_topic_model = textmodel_lda(CJEU_dfm_tidy_trimmed, K)
saveRDS(cjeu_topic_model, file = "../data/cjeu_topic_model.rds")
setwd("~/Library/CloudStorage/OneDrive-Humboldt-UniversitaetzuBerlin,CMS/PhD/Courts and Judges/courts_and_judges/introduction_to_R")
saveRDS(cjeu_topic_model, file = "../data/cjeu_topic_model.rds")
# List 20 terms that define each topic
terms(cjeu_topic_model, n = 20)
