upper_tmrw = quantile(tomorrow, 0.975)
)
View(humidity_model_df)
View(humidity_model_df)
View(humidity_bike_predict)
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(40000, mean = avg_day, sd = sigma))
humidity_bike_predict |>
summarise(
lower_avg = quantile(avg_day, 0.025),
upper_avg = quantile(avg_day, 0.975),
lower_tmrw = quantile(tomorrow, 0.025),
upper_tmrw = quantile(tomorrow, 0.975)
)
humidity_bike_predict2 =
posterior_predict(humidity_model, newdata = data.frame(humidity = 90))
posterior_interval(humidity_bike_predict2, prob = 0.95)
humidity_bike_predict |>
summarize(
lower_avg = quantile(avg_day, 0.025),
upper_avg = quantile(avg_day, 0.975),
lower_tmrw = quantile(tomorrow, 0.025),
upper_tmrw = quantile(tomorrow, 0.975)
)
View(humidity_model_df)
set.seed(84735)
humidity_bike_predict2 =
posterior_predict(humidity_model, newdata = data.frame(humidity = 90))
posterior_interval(humidity_bike_predict2, prob = 0.95)
# This includes the outcome - the posterior simulation
humidity_model_posterior = stan_glm(rides ~ humidity, data = bikes,
family = gaussian,
prior = normal(10, 3),
prior_intercept = normal(5000, 1500),
prior_aux = exponential(1/2000),
chains = 5, iter = 8000*2, seed = 84735)
humidity_model_df = humidity_model_posterior |>
as_tibble()
humidity_model_posterior |>
tidy(effects = c("fixed", "aux"),
conf.int = TRUE, conf.level = 0.95)
# Posterior model diagnostics
humidity_model_prior |>
tidy(effects = c("fixed", "aux"),
conf.int = TRUE, conf.level = 0.80)
# Prediction
# Prediction "by hand"
humidity_bike_predict = humidity_model_df |>
mutate(avg_day = `(Intercept)` + humidity*90,
tomorrow = rnorm(40000, mean = avg_day, sd = sigma))
humidity_bike_predict |>
ggplot() +
geom_density(aes(x = avg_day)) +
geom_density(aes(x = tomorrow)) +
labs(x = "riders")
humidity_bike_predict |>
summarize(
lower_avg = quantile(avg_day, 0.025),
upper_avg = quantile(avg_day, 0.975),
lower_tmrw = quantile(tomorrow, 0.025),
upper_tmrw = quantile(tomorrow, 0.975)
)
humidity_bike_predict2 =
posterior_predict(humidity_model_posterior, newdata = data.frame(humidity = 90))
posterior_interval(humidity_bike_predict2, prob = 0.95)
set.seed(84735)
humidity_bike_predict2 =
posterior_predict(humidity_model_posterior, newdata = data.frame(humidity = 90))
posterior_interval(humidity_bike_predict2, prob = 0.95)
set.seed(84735)
humidity_bike_predict2 =
posterior_predict(humidity_model_posterior, newdata = data.frame(humidity = 90))
pp_check(humidity_model_posterior, nreps = 50) +
xlab("rides")
complete_prediction = posterior_predict(humidity_model_posterior, newdata = bikes)
dim(complete_prediction)
?ppc_intervals
ppc_intervals(y = bikes$rides, yrep = complete_prediction, x = bikes$humidity)
?ppc_intervals
ppc_intervals(y = bikes$rides, yrep = complete_prediction, x = bikes$humidity,
prob = 0.5, prob_outer = 0.95)
prediction_summary(model = humidity_model_prior, data = bikes)
set.seed(84735)
prediction_summary(model = humidity_model_prior, data = bikes)
knitr::opts_chunk$set(eval = FALSE)
# Install packages
# install.packages("tidyverse")
# install.packages("lubridate")
# install.packages("tidytext")
# install.packages("quanteda")
# install.packages("quanteda.textstats")
# install.packages("quanteda.textmodels")
# install.packages("ldatuning")
# install.packages("seededlda")
# Load packages
library(tidyverse)
library(lubridate)
library(tidytext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)
library(ldatuning)
library(seededlda)
# Load data dynamically from a github page (should've done this earlier)
load(url("https://github.com/jfjelstul/law-and-courts-workshop/raw/master/data/text_corpus.RData")) %>% as_tibble()
bvg_text_corpus = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/bvg_text_corpus.rds")) %>% as_tibble()
# Creating a dtm/dfm multiple ways---
# Stop words - they are words that you think carry little to zero meaning in terms of distinguishing topics from each other
to_remove = tibble(
word = c(
"article", "court", "paragraph", "judgment", "case",
"proceedings", "apeal", "application",
"directive", "regulation", "law",
"member", "state", "states", "commission", "european", "union"
)
)
# Add the typical english stop words
to_remove = bind_rows(
get_stopwords() %>% select(word),
to_remove
)
# Quanteda
quanteda_corpus = text_corpus %>%
select(ecli, text) %>%
group_by(ecli) %>%
summarise(text = paste(text, collapse = " ")) %>%
corpus(
docid_field = "ecli",
text_field = "text"
)
CJEU_dfm = tokens(quanteda_corpus, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE) %>%
tokens_select(pattern = to_remove$word, valuetype = "regex", selection = "remove", min_nchar=2L) %>%   dfm() %>%
dfm_trim(min_termfreq = 5)
# number of topics
K = 10
cjeu_topic_model = textmodel_lda(CJEU_dfm, K)
saveRDS(cjeu_topic_model, file = "../data/cjeu_topic_model.rds")
# List 20 terms that define each topic
terms(cjeu_topic_model, n = 20)
# Stop words - they are words that you think carry little to zero meaning in terms of distinguishing topics from each other
to_remove = tibble(
word = c(
"article", "court", "paragraph", "judgment", "case",
"proceedings", "apeal", "application",
"directive", "regulation", "law",
"member", "state", "states", "commission", "european", "union", "eu", "eu:c"
)
)
to_remove = tibble(
word = c(
"article", "court", "paragraph", "judgment", "case",
"proceedings", "apeal", "application",
"directive", "regulation", "law",
"member", "state", "states", "commission", "european", "union", "eu", "eu:c"
)
)
# Add the typical english stop words
to_remove = bind_rows(
get_stopwords() %>% select(word),
to_remove
)
# Quanteda
quanteda_corpus = text_corpus %>%
select(ecli, text) %>%
group_by(ecli) %>%
summarise(text = paste(text, collapse = " ")) %>%
corpus(
docid_field = "ecli",
text_field = "text"
)
CJEU_dfm = tokens(quanteda_corpus, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE) %>%
tokens_select(pattern = to_remove$word, valuetype = "regex", selection = "remove", min_nchar=2L) %>%   dfm() %>%
dfm_trim(min_termfreq = 5)
# number of topics
K = 10
cjeu_topic_model = textmodel_lda(CJEU_dfm, K)
# List 20 terms that define each topic
terms(cjeu_topic_model, n = 20)
saveRDS(cjeu_topic_model, file = "../data/cjeu_topic_model.rds")
length(unique(text_corpus$ecli))
# List 20 terms that define each topic
terms(cjeu_topic_model, n = 20)
# List 20 terms that define each topic
terms(cjeu_topic_model, n = 20)
saveRDS(cjeu_topic_model, file = "../data/cjeu_topic_model.rds")
cjeu_topic_model = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/cjeu_topic_model.rds"))
CJEU_dfm
knitr::opts_chunk$set(eval = FALSE)
# Install packages
# install.packages("tidyverse")
# install.packages("lubridate")
# install.packages("tidytext")
# install.packages("quanteda")
# install.packages("quanteda.textstats")
# install.packages("quanteda.textmodels")
# install.packages("ldatuning")
# install.packages("seededlda")
# install.packages("udpipe")
# install.packages("parallel")
# Load packages
library(parallel)
library(tidyverse)
library(lubridate)
library(tidytext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)
library(ldatuning)
library(seededlda)
library(udpipe)
# Load data dynamically from a github page (should've done this earlier)
load(url("https://github.com/jfjelstul/law-and-courts-workshop/raw/master/data/text_corpus.RData")) %>% as_tibble()
NSS_df = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/NSS_df.rds"))
library(parallel)
library(tidyverse)
library(lubridate)
library(tidytext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)
library(ldatuning)
library(seededlda)
library(udpipe)
# Load data dynamically from a github page (should've done this earlier)
load(url("https://github.com/jfjelstul/law-and-courts-workshop/raw/master/data/text_corpus.RData")) %>% as_tibble()
string = "courts and judges"
string %>%
str_extract(pattern = "courts")
string %>%
str_detect(pattern = "duck")
string %>%
str_detect(pattern = "and")
# Run the first
wiki = data.frame(wiki_text = c("Lee Epstein is a political scientist", "Arnold Schwarzenegger is an american actor"))
wiki = data.frame(wiki_text = c("Lee Epstein is a political scientist", "Arnold Schwarzenegger is an american actor"))
# And then run this and try to understand what just happened
wiki = string %>%
mutate(
name = wiki_text %>%
str_extract(pattern = "[A-Z][a-z]+\\s[A-Z][a-z]+")
)
library(parallel)
library(tidyverse)
library(lubridate)
library(tidytext)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textmodels)
library(ldatuning)
library(seededlda)
library(udpipe)
# And then run this and try to understand what just happened
wiki = string %>%
mutate(
name = wiki_text %>%
str_extract(pattern = "[A-Z][a-z]+\\s[A-Z][a-z]+")
)
# And then run this and try to understand what just happened
wiki = wiki %>%
mutate(
name = wiki_text %>%
str_extract(pattern = "[A-Z][a-z]+\\s[A-Z][a-z]+")
)
# Run the first
wiki = data.frame(wiki_text = c("Lee Epstein is a political scientist", "Arnold Schwarzenegger is an american actor"))
View(wiki)
# And then run this and try to understand what just happened
wiki = wiki %>%
mutate(
name = wiki_text %>%
str_extract(pattern = "[A-Z][a-z]+\\s[A-Z][a-z]+")
)
View(wiki)
wiki = data.frame(wiki_text = c("Lee Epstein is a political scientist", "Arnold Schwarzenegger is an american actor", "D. Trump is a former US president"))
View(wiki)
View(wiki)
View(text_corpus)
judge_rapporteur = text_corpus %>%
select(ecli, text) %>%
filter(str_detect(text, "^composed of")) %>%
as_tibble() %>%
slice_tail(n = 3)
View(judge_rapporteur)
View(text_corpus)
View(judge_rapporteur)
View(judge_rapporteur)
View(wiki)
?slice_tail
NSS_df = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/NSS_df.rds"))
View(NSS_df)
# Check the ability distribution
NSS_df %>%
ggplot() +
geom_density(aes(x = ability, group = phd, color = phd))
bvg_text_corpus = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/bvg_text_corpus.rds")) %>%
as_tibble() %>%
slice_head(n = 100)
View(bvg_text_corpus)
n.cores = detectCores() - 2
ud_model = udpipe_download_model(language = "german")
ud_model = udpipe_load_model(ud_model$file_model)
# Parallesized
start = Sys.time()
bvg_udpipe = udpipe(x = bvg_text_corpus, object = ud_model, parallel.cores = n.cores) %>%
as_tibble() %>%
select(c(doc_id, paragraph_id, start, end, token, lemma, upos))
select()
# Parallesized
start = Sys.time()
bvg_udpipe = udpipe(x = bvg_text_corpus, object = ud_model, parallel.cores = n.cores) %>%
as_tibble() %>%
select(c(doc_id, paragraph_id, start, end, token, lemma, upos))
end = Sys.time()
View(bvg_udpipe)
cjeu_topic_model = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/cjeu_topic_model.rds"))
# List 20 terms that define each topic
terms(cjeu_topic_model, n = 20)
View(NSS_df)
xfun::pkg_attach2("tidyverse","udpipe", "foreach", "doParallel")
library(skimr)
library(tidymodels)
library(tidyverse)
data = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/rand_forest_exercise.rds")) %>%
drop_na() %>%
mutate(successful = as.character(successful)) %>%
mutate(across(where(is.character), as.factor))
skim(data)
view(data)
data_split = initial_split(data, prop = 3/4)
train_data = training(data_split)
test_data  = testing(data_split)
# Prepare data for k-fold cross-validation
folds = vfold_cv(train_data, v = 6)
unique(data$plaintiff)
# Not all features of this data set are helpful, determine which ones would you like to go in in your formula
rec = recipe(successful ~ ., data = train_data) %>%
update_role(iuropa_decision_id, new_role = "ID") %>%
update_role(successful, new_role = "outcome") %>%
update_role(all_predictors(), new_role = "predictor") %>%
step_unknown()
rec
mod_tune = rand_forest(
mtry = tune(),
trees = 1000,
min_n = tune()
) %>%
set_mode("classification") %>%
set_engine("ranger")
wfl_tune = workflow() %>%
add_recipe(rec) %>%
add_model(mod_tune)
fit_tune = tune_grid(
wfl_tune,
resamples = folds,
grid = 20
)
# Plot the tuning result
fit_tune %>%
collect_metrics() %>%
filter(.metric == "roc_auc") %>%
select(mean, min_n, mtry) %>%
pivot_longer(min_n:mtry,
values_to = "value",
names_to = "parameter"
) %>%
ggplot(aes(value, mean, color = parameter)) +
geom_point(show.legend = FALSE) +
facet_wrap(~parameter, scales = "free_x") +
labs(x = NULL, y = "AUC")
rf_grid <- grid_regular(
mtry(range = c(10, 20)),
min_n(range = c(2, 8)),
levels = 5
)
regular_res <- tune_grid(
wfl_tune,
resamples = folds,
grid = rf_grid
)
best_auc <- select_best(regular_res, "roc_auc")
final_rf <- finalize_model(
tune_spec,
best_auc
)
final_rf <- finalize_model(
wfl_tune,
best_auc
)
final_rf <- finalize_model(
wfl_tune,
best_auc
)
View(regular_res)
View(best_auc)
final_rf <- finalize_model(
mod_tune,
best_auc
)
final_rf <- finalize_model(
mod_tune,
best_auc
)
final_wf <- workflow() %>%
add_recipe(rec) %>%
add_model(final_rf)
final_res <- final_wf %>%
last_fit(data_split)
final_res %>%
collect_metrics()
?rand_forest
library(skimr)
library(tidymodels)
library(tidyverse)
library(themis)
install.packages("kernlab", "xgboost", "tidymodels", "tidyverse", "skimr", "ranger")
install.packages("kernlab", "xgboost", "tidymodels", "tidyverse", "skimr", "ranger")
xfun::pkg_attach2("kernlab", "xgboost", "tidymodels", "tidyverse", "skimr", "ranger")
xfun::pkg_attach2("kernlab", "xgboost", "tidymodels", "tidyverse", "skimr", "ranger")
?rand_forest
knitr::opts_chunk$set(eval = FALSE)
# install.packages("kernlab", "tidymodels", "tidyverse", "skimr", "ranger", "xgboost")
library(skimr)
library(tidymodels)
library(tidyverse)
library(themis)
bvg_text_corpus = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/bvg_text_corpus.rds")) %>%
as_tibble()
n.cores = detectCores() - 2
library(parallel)
library(udpipe)
n.cores = detectCores() - 2
ud_model = udpipe_download_model(language = "german")
ud_model = udpipe_load_model(ud_model$file_model)
# Parallesized
start = Sys.time()
bvg_udpipe = udpipe(x = bvg_text_corpus, object = ud_model, parallel.cores = n.cores) %>%
as_tibble() %>%
select(c(doc_id, paragraph_id, start, end, token, lemma, upos))
library(skimr)
library(tidymodels)
library(tidyverse)
library(themis)
library(udpipe)
library(parallel)
setwd("~/Library/CloudStorage/OneDrive-Humboldt-UniversitaetzuBerlin,CMS/PhD/Courts and Judges/courts_and_judges/introduction_to_R")
bvg_text_corpus = readRDS(url("https://github.com/stepanpaulik/courts_and_judges/raw/main/data/bvg_text_corpus.rds")) %>%
as_tibble()
# Download the language model and determine the number of cores used for the process
n.cores = detectCores() - 2
ud_model = udpipe_download_model(language = "german")
ud_model = udpipe_load_model(ud_model$file_model)
start = Sys.time()
bvg_udpipe = udpipe(x = bvg_text_corpus, object = ud_model, parallel.cores = n.cores) %>%
as_tibble() %>%
select(c(doc_id, paragraph_id, start, end, token, lemma, upos))
end = Sys.time()
View(bvg_udpipe)
saveRDS(bvg_udpipe, "../data/bvg_text_corpus_udpipe.rds")
bvg_text_corpus_lemma = glue_udpipe(data = bvg_udpipe)
# Put the UDPipe back together
glue_udpipe = function(data){
output = data %>%
group_by(doc_id) %>%
filter(str_length(lemma) >= 3) %>%
filter(
!str_detect(lemma, "[0-9]")
) %>%
filter(upos != "PUNCT") %>%
summarise(text = paste(lemma, collapse = " ") %>% # Normalise and tidy the text
str_to_lower() %>%
str_squish())
return(output)
}
bvg_text_corpus_lemma = glue_udpipe(data = bvg_udpipe)
View(bvg_text_corpus_lemma)
bvg_udpipe %>% slice_head(n = 1000) %>% saveRDS(., "../data/bvg_text_corpus_udpipe.rds")
saveRDS(bvg_text_corpus_lemma, "../data/bvg_text_corpus_lemma.rds")
View(bvg_text_corpus_lemma)
library(word2vec)
word2vec(x = bvg_text_corpus_lemma, type = "cbow")
bvg_embeddings = word2vec(x = bvg_text_corpus_lemma, type = "cbow")
?word2vec
bvg_embeddings = word2vec(x = bvg_text_corpus_lemma$text, type = "cbow")
View(bvg_embeddings)
View(bvg_text_corpus_lemma)
predict(bvg_embeddings, c("senat", "urteil"), type = "nearest", top_n = 5)
predict(bvg_embeddings, c("straftater", "urteil"), type = "nearest", top_n = 5)
predict(bvg_embeddings, c("straftäter", "urteil"), type = "nearest", top_n = 5)
x = emb[c("strafen", "straftat", "senat"), ]
x = bvg_embeddings[c("strafen", "straftat", "senat"), ]
x = as.matrix(bvg_embeddings)[c("strafen", "straftat", "senat"), ]
emb = as.matrix(bvg_embeddings)
x = emb[c("strafen", "straftat", "senat"), ]
x = emb[c("straf", "straftat", "senat"), ]
word2vec_similarity(x, x)
emb["straf"] - emb["straf"]
emb["straf"] - emb["straftat"]
emb["straf",] - emb["straftat",]
emb["straf",] - emb["straftat",] + emb["senat",]
emb["man"]
emb["mann"]
emb["mann"]
emb["manner"]
emb["frau"]
emb["weib"]
predict(bvg_embeddings, emb["straf",] - emb["straftat",] + emb["senat",], type = "nearest", top_n = 5)
?predict
predict(bvg_embeddings, emb["straf",] - emb["straftat",] + emb["gericht",], type = "nearest", top_n = 5)
predict(bvg_embeddings, emb["straf",] - emb["entschädigung",] + emb["straftat",], type = "nearest", top_n = 5)
