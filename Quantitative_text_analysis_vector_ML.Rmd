---
title: "Quantitative_text_analysis_vector_ML"
author: "Štěpán Paulík"
date: "2023-06-17"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE)

library(skimr)
library(tidymodels)
library(tidyverse)
```

## Overview

Today we will do a brief overview, or rather an introduction to, a potential usage of machine learning in social science/text data analysis. We have already briefly gone over the standard "data science" or data based research workflow, which looks like this: 

![Standard datascience workflow](https://d33wubrfki0l68.cloudfront.net/795c039ba2520455d833b4034befc8cf360a70ba/558a5/diagrams/data-science-explore.png)

Up until now, we have mainly focused on the import, tidying and transformation phases. We have imported data either locally through a relative path or via an internet URL (but there are countless ways including SQL database usage, loading from cloud services etc.), we have learned how what tidy data are and what are the step to reach them and last two weeks we have covered some basic data transformation tasks (the mutate() and summarise() functions, data mining from text via regex etc.).

However, that is hardly ever a goal in itself of data analysis. Whether it's in private sector or in university research, usually at least some form of visualization (even if of descriptive statistics) is a minimum. More often than not you need to process your data with a model, whether to infere something about a process or to make predictions.

## Machine Learning

### Overview
Machine learning algorithms build a model, i.e., learn certain patterns, on a training data and then apply the model to previously unseed data to make predictions.

Even the in social science omnipresent *linear regression* is a form of statistical model and machine learning algorithm, even if a pretty basic one (which is its advantage for social science: it makes it easily interpretable). Think of your inference about the intercepts in linear regression and your conclusion about casual inference as as the model's application to previously unseen data (i.e., you mostly train your linear regression only on a small sample to make conclusions about the whole population).

As we have already touched upon, there are two types of machine learning algorithms

(1) unsupervised, where no labels/classes are given before the training, thus the algorithm discerns the structure/patterns on its own
(2) supervised, where a researcher gives an input in the form of labelled/annotated data before hand, and the algorithm learns a method to structure the input to the desired output (labels, classes)

The former form of ML is often used for exploratory data analysis (like the unsupervised topic modeling we have covered), whereas the latter can be used for classification, prediction, inference etc.
### Supervised Machine Learning
Today our main topic will be supervised machine learning. We will discuss the workflow in general with focus on *tidymodels*, the tidy syntax applied to machine learning modeling.

For any model, you need to specify the formula (i.e., what is your class that you want to predict and what are the features used to predict that class), the model (today we will look at *random forests, support vector machines and gradient boosted trees*)

Tidymodels, unlike the base R syntax or Python syntax, splits up the machine learning model into multiple chunks, which allows you to modulate them as you wish without having to rewrite the whole code. 

Thus, the tidymodels workflow proceeds as follow:
(1) You create a recipe, where you specify the formula as well as the role played by each function (i.e., whether it's the outcome class, ID for the observation or the predictor/feature)
(2) You specify the model, which includes the model's engine, goal (classification or regression), and most importantly parameters
(3) You create a workflow, where you merge these two together
(4) You train the model on training data, tune the parameters, do diagnosis etc.
(5) You apply it to previously unseen test data
(6) You diagnose the model

#### Training vs test data
Before you train any model, you split your labeled data into training and testing data. You then train the model on the training data and run the various diagnoses on the test data. It is starting to become a good practice in research that not only the model but also the researcher does not see the test data before running the diagnosis.

#### Example from scratch - the Lee Epstein dissents research replicated on the Czech Constitutional Court

We will now apply the whole process on my example data. The data are hand annotated 200 decisions of the constitutional court. 

##### Create the recipe
``` {r tidymodels}
data = readRDS("../data/judgments_annotated_doc2vec.rds")


```


## Vectorisation
